{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwD48aWrAxPc"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q keras-tuner\n"
      ],
      "metadata": {
        "id": "ZTyHQErgCRP1",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import os\n",
        "import keras_tuner as kt\n",
        "\n",
        "# === Load and preprocess data ===\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "df = pd.read_csv(os.path.join(data_dir, \"trainingData_filtered.csv\"))\n",
        "\n",
        "# Prepare WAPs and Labels\n",
        "wap_columns = [col for col in df.columns if \"WAP\" in col]\n",
        "location_columns = [\"LONGITUDE\", \"LATITUDE\"]\n",
        "building_floor_columns = [\"BUILDINGID\", \"FLOOR\"]\n",
        "\n",
        "# Remove low-variance WAPs\n",
        "wap_std = df[wap_columns].std()\n",
        "useful_waps = wap_std[wap_std > 0.01].index.tolist()\n",
        "df = df[useful_waps + location_columns + building_floor_columns]\n",
        "\n",
        "# One-hot encode metadata\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "building_floor_onehot = encoder.fit_transform(df[building_floor_columns])\n",
        "onehot_columns = encoder.get_feature_names_out(building_floor_columns)\n",
        "df[onehot_columns] = building_floor_onehot\n",
        "\n",
        "# Final features and labels\n",
        "X_all = df[useful_waps + list(onehot_columns)].values\n",
        "Y_all = df[location_columns].values\n",
        "num_rssi = len(useful_waps)\n",
        "\n",
        "# Normalize coordinates\n",
        "Y_min = Y_all.min(axis=0)\n",
        "Y_max = Y_all.max(axis=0)\n",
        "Y_all_scaled = (Y_all - Y_min) / (Y_max - Y_min)\n",
        "\n",
        "# === Sequence generation ===\n",
        "def generate_sequences(T):\n",
        "    max_dist = 1.5\n",
        "    dist_matrix = euclidean_distances(Y_all, Y_all)\n",
        "    X_seq, Y_seq = [], []\n",
        "\n",
        "    for _ in range(5000):\n",
        "        start_idx = np.random.randint(0, len(df))\n",
        "        traj_x, traj_y = [X_all[start_idx]], [Y_all_scaled[start_idx]]\n",
        "        current_idx = start_idx\n",
        "        for _ in range(T - 1):\n",
        "            neighbors = np.where(dist_matrix[current_idx] <= max_dist)[0]\n",
        "            if len(neighbors) == 0:\n",
        "                break\n",
        "            next_idx = np.random.choice(neighbors)\n",
        "            traj_x.append(X_all[next_idx])\n",
        "            traj_y.append(Y_all_scaled[next_idx])\n",
        "            current_idx = next_idx\n",
        "        if len(traj_x) == T:\n",
        "            X_seq.append(traj_x)\n",
        "            Y_seq.append(traj_y)\n",
        "    return np.array(X_seq), np.array(Y_seq)\n",
        "\n",
        "# === Loss Function ===\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)))\n",
        "\n",
        "# === Model Builder ===\n",
        "def build_model(hp):\n",
        "    T = hp.Choice(\"seq_length\", [10, 15, 20])\n",
        "    lstm_units = hp.Choice(\"lstm_units\", [64, 128])\n",
        "    hp.Choice(\"batch_size\", [32, 64, 128])\n",
        "\n",
        "    model_input = tf.keras.Input(shape=(T, X_all.shape[1]))\n",
        "    x = tf.keras.layers.BatchNormalization()(model_input)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm_units, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    output = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(2))(x)\n",
        "\n",
        "    model = tf.keras.Model(inputs=model_input, outputs=output)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "                  loss=euclidean_distance_loss,\n",
        "                  metrics=[euclidean_distance_loss])\n",
        "    return model\n",
        "\n",
        "# === Custom Trial Runner ===\n",
        "def custom_run_trial(tuner, trial):\n",
        "    hp = trial.hyperparameters\n",
        "    T = hp.get(\"seq_length\")\n",
        "    batch_size = hp.get(\"batch_size\")\n",
        "\n",
        "    X_seq, Y_seq = generate_sequences(T)\n",
        "    X_seq[:, :, :num_rssi] = (X_seq[:, :, :num_rssi] + 105) / 105\n",
        "    X_train, X_val, Y_train, Y_val = train_test_split(X_seq, Y_seq, test_size=0.2)\n",
        "\n",
        "    model = tuner.hypermodel.build(hp)\n",
        "\n",
        "    history = model.fit(X_train, Y_train,\n",
        "                        validation_data=(X_val, Y_val),\n",
        "                        epochs=10,\n",
        "                        batch_size=batch_size,\n",
        "                        verbose=0)\n",
        "\n",
        "    val_loss = model.evaluate(X_val, Y_val, verbose=0)\n",
        "    tuner.oracle.update_trial(trial.trial_id, {'val_euclidean_distance_loss': val_loss[0]})\n",
        "\n",
        "# === Subclass the Tuner ===\n",
        "class MyTuner(kt.RandomSearch):\n",
        "    def run_trial(self, trial, *args, **kwargs):\n",
        "        return custom_run_trial(self, trial)\n",
        "\n",
        "# === Initialize and Run Tuner ===\n",
        "tuner = MyTuner(\n",
        "    build_model,\n",
        "    objective=kt.Objective('val_euclidean_distance_loss', direction='min'),\n",
        "    max_trials=10,\n",
        "    executions_per_trial=1,\n",
        "    overwrite=True,\n",
        "    directory='kt_tuner_logs',\n",
        "    project_name='indoor_lstm_tuning'\n",
        ")\n",
        "\n",
        "tuner.search_space_summary()\n",
        "tuner.search()\n",
        "\n",
        "# === Best Results ===\n",
        "best_hp = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "print(\"\\nâœ… Best Hyperparameters:\")\n",
        "print(f\"Sequence length: {best_hp['seq_length']}\")\n",
        "print(f\"LSTM units: {best_hp['lstm_units']}\")\n",
        "print(f\"Batch size: {best_hp['batch_size']}\")\n"
      ],
      "metadata": {
        "id": "Kyzf8ssTA2uz",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy.spatial.distance import euclidean\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# === Setup ===\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# === Load Filtered Data ===\n",
        "df = pd.read_csv(os.path.join(data_dir, \"trainingData_filtered.csv\"))\n",
        "\n",
        "# Identify WAP, location, and metadata columns\n",
        "wap_columns = [col for col in df.columns if \"WAP\" in col]\n",
        "location_columns = [\"LONGITUDE\", \"LATITUDE\"]\n",
        "building_floor_columns = [\"BUILDINGID\", \"FLOOR\"]\n",
        "\n",
        "# Filter out WAPs with low variance\n",
        "wap_std = df[wap_columns].std()\n",
        "useful_waps = wap_std[wap_std > 0.01].index.tolist()\n",
        "df = df[useful_waps + location_columns + building_floor_columns]\n",
        "\n",
        "# One-hot encode BUILDINGID and FLOOR\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "building_floor_onehot = encoder.fit_transform(df[building_floor_columns])\n",
        "onehot_columns = encoder.get_feature_names_out(building_floor_columns)\n",
        "df[onehot_columns] = building_floor_onehot\n",
        "\n",
        "# Combine RSSI + one-hot features\n",
        "feature_columns = useful_waps + list(onehot_columns)\n",
        "X_all = df[feature_columns].values\n",
        "Y_all = df[location_columns].values\n",
        "\n",
        "# === Generate Enhanced Trajectories ===\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "T = 20  # best sequence length\n",
        "v_max = 1.5  # m/s\n",
        "delta_t = 1.0\n",
        "max_dist = v_max * delta_t\n",
        "\n",
        "dist_matrix = euclidean_distances(Y_all, Y_all)\n",
        "X_sequences = []\n",
        "Y_sequences = []\n",
        "\n",
        "for _ in range(5000):\n",
        "    start_idx = np.random.randint(0, len(df))\n",
        "    traj_x = [X_all[start_idx]]\n",
        "    traj_y = [Y_all[start_idx]]\n",
        "    current_idx = start_idx\n",
        "\n",
        "    for _ in range(T - 1):\n",
        "        neighbors = np.where(dist_matrix[current_idx] <= max_dist)[0]\n",
        "        if len(neighbors) == 0:\n",
        "            break\n",
        "        next_idx = np.random.choice(neighbors)\n",
        "        traj_x.append(X_all[next_idx])\n",
        "        traj_y.append(Y_all[next_idx])\n",
        "        current_idx = next_idx\n",
        "\n",
        "    if len(traj_x) == T:\n",
        "        X_sequences.append(traj_x)\n",
        "        Y_sequences.append(traj_y)\n",
        "\n",
        "X_sequences = np.array(X_sequences)\n",
        "Y_sequences = np.array(Y_sequences)\n",
        "\n",
        "# np.save(os.path.join(data_dir, \"X_sequences_enhanced.npy\"), X_sequences)\n",
        "# np.save(os.path.join(data_dir, \"Y_sequences_enhanced.npy\"), Y_sequences)\n",
        "\n",
        "# print(\"âœ… Enhanced sequences generated and saved.\")\n",
        "\n",
        "# === Normalize Inputs ===\n",
        "X = X_sequences\n",
        "Y = Y_sequences\n",
        "\n",
        "num_rssi = len(useful_waps)\n",
        "X[:, :, :num_rssi] = (X[:, :, :num_rssi] + 105) / 105  # Normalize RSSI\n",
        "\n",
        "Y_min = Y.min(axis=(0, 1))\n",
        "Y_max = Y.max(axis=(0, 1))\n",
        "Y_scaled = (Y - Y_min) / (Y_max - Y_min)\n",
        "\n",
        "# np.save(os.path.join(data_dir, \"Y_min_1.npy\"), Y_min)\n",
        "# np.save(os.path.join(data_dir, \"Y_max_1.npy\"), Y_max)\n",
        "\n",
        "# === Train/Test Split ===\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y_scaled, test_size=0.2, random_state=42)\n",
        "\n",
        "# === Loss Function ===\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    return tf.reduce_mean(tf.sqrt(tf.reduce_sum(tf.square(y_true - y_pred), axis=-1)))\n",
        "\n",
        "# === Build Optimized Model ===\n",
        "def build_model(input_shape, output_dim):\n",
        "    inputs = tf.keras.Input(shape=input_shape)\n",
        "    x = tf.keras.layers.BatchNormalization()(inputs)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = tf.keras.layers.Dropout(0.3)(x)\n",
        "    outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(output_dim))(x)\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "                  loss=euclidean_distance_loss,\n",
        "                  metrics=[euclidean_distance_loss])\n",
        "    return model\n",
        "\n",
        "model = build_model(X_train.shape[1:], output_dim=2)\n",
        "\n",
        "# === Train Model ===\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=150,\n",
        "    batch_size=64,  # best batch size\n",
        "    # callbacks=[early_stop]\n",
        ")\n",
        "\n",
        "# === Save Model ===\n",
        "model.save(os.path.join(data_dir, \"hyper_model_with_building_floor.keras\"))\n",
        "print(\"âœ… Model with building+floor saved.\")\n",
        "\n",
        "# === Evaluate ===\n",
        "Y_pred_scaled = model.predict(X_val)\n",
        "Y_pred = Y_pred_scaled * (Y_max - Y_min) + Y_min\n",
        "Y_true = Y_val * (Y_max - Y_min) + Y_min\n",
        "\n",
        "def average_localization_error(y_true, y_pred):\n",
        "    batch, time_steps, _ = y_true.shape\n",
        "    total_error = sum(\n",
        "        euclidean(y_true[i][t], y_pred[i][t])\n",
        "        for i in range(batch) for t in range(time_steps)\n",
        "    )\n",
        "    return total_error / (batch * time_steps)\n",
        "\n",
        "error_meters = average_localization_error(Y_true, Y_pred)\n",
        "print(f\"ðŸ“ Final Average Localization Error: {error_meters:.2f} meters\")\n",
        "\n",
        "# === Plot Loss ===\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title(\"Training Loss (Euclidean Distance)\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UI3fEuzqBr7X",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=150,\n",
        "    batch_size=64,  # best batch size\n",
        "    # callbacks=[early_stop]\n",
        ")"
      ],
      "metadata": {
        "id": "9lRVazOVeTqn",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(os.path.join(data_dir, \"hyper_model_with_building_floor.keras\"))"
      ],
      "metadata": {
        "id": "qq6cgi9ljUG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_scaled = model.predict(X_val)\n",
        "Y_pred = Y_pred_scaled * (Y_max - Y_min) + Y_min\n",
        "Y_true = Y_val * (Y_max - Y_min) + Y_min\n",
        "\n",
        "def average_localization_error(y_true, y_pred):\n",
        "    batch, time_steps, _ = y_true.shape\n",
        "    total_error = sum(\n",
        "        euclidean(y_true[i][t], y_pred[i][t])\n",
        "        for i in range(batch) for t in range(time_steps)\n",
        "    )\n",
        "    return total_error / (batch * time_steps)\n",
        "\n",
        "error_meters = average_localization_error(Y_true, Y_pred)\n",
        "print(f\"ðŸ“ Final Average Localization Error: {error_meters:.2f} meters\")"
      ],
      "metadata": {
        "id": "jk2ycuSS39sP",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=50,\n",
        "    batch_size=64,  # best batch size\n",
        "    # callbacks=[early_stop]\n",
        ")"
      ],
      "metadata": {
        "id": "5ss5fVjw4GGj",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(os.path.join(data_dir, \"hyper_model_with_building_floor.keras\"))"
      ],
      "metadata": {
        "id": "_NwQT5aL-PvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_scaled = model.predict(X_val)\n",
        "Y_pred = Y_pred_scaled * (Y_max - Y_min) + Y_min\n",
        "Y_true = Y_val * (Y_max - Y_min) + Y_min\n",
        "\n",
        "def average_localization_error(y_true, y_pred):\n",
        "    batch, time_steps, _ = y_true.shape\n",
        "    total_error = sum(\n",
        "        euclidean(y_true[i][t], y_pred[i][t])\n",
        "        for i in range(batch) for t in range(time_steps)\n",
        "    )\n",
        "    return total_error / (batch * time_steps)\n",
        "\n",
        "error_meters = average_localization_error(Y_true, Y_pred)\n",
        "print(f\"ðŸ“ Final Average Localization Error: {error_meters:.2f} meters\")"
      ],
      "metadata": {
        "id": "ZA7KAKqDFY1y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train, Y_train,\n",
        "    validation_data=(X_val, Y_val),\n",
        "    epochs=50,\n",
        "    batch_size=64,  # best batch size\n",
        "    # callbacks=[early_stop]\n",
        ")"
      ],
      "metadata": {
        "id": "FGkxju_RFa0g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(os.path.join(data_dir, \"hyper_model_with_building_floor.keras\"))"
      ],
      "metadata": {
        "id": "SJvkq4cwFiUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_pred_scaled = model.predict(X_val)\n",
        "Y_pred = Y_pred_scaled * (Y_max - Y_min) + Y_min\n",
        "Y_true = Y_val * (Y_max - Y_min) + Y_min\n",
        "\n",
        "def average_localization_error(y_true, y_pred):\n",
        "    batch, time_steps, _ = y_true.shape\n",
        "    total_error = sum(\n",
        "        euclidean(y_true[i][t], y_pred[i][t])\n",
        "        for i in range(batch) for t in range(time_steps)\n",
        "    )\n",
        "    return total_error / (batch * time_steps)\n",
        "\n",
        "error_meters = average_localization_error(Y_true, Y_pred)\n",
        "print(f\"ðŸ“ Final Average Localization Error: {error_meters:.2f} meters\")"
      ],
      "metadata": {
        "id": "HZN8KCmaOQqJ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "# Define the custom loss function\n",
        "def euclidean_distance_loss(y_true, y_pred):\n",
        "    return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
        "\n",
        "# Now load the model with custom_objects\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/hyper_model_with_building_floor.keras'\n",
        "model = load_model(model_path, custom_objects={'euclidean_distance_loss': euclidean_distance_loss})\n"
      ],
      "metadata": {
        "id": "granjlWQOSH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from scipy.spatial.distance import euclidean\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# === Setup ===\n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks\"\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# === Load Filtered Data ===\n",
        "df = pd.read_csv(os.path.join(data_dir, \"trainingData_filtered.csv\"))\n",
        "\n",
        "# Identify WAP, location, and metadata columns\n",
        "wap_columns = [col for col in df.columns if \"WAP\" in col]\n",
        "location_columns = [\"LONGITUDE\", \"LATITUDE\"]\n",
        "building_floor_columns = [\"BUILDINGID\", \"FLOOR\"]\n",
        "\n",
        "# Filter out WAPs with low variance\n",
        "wap_std = df[wap_columns].std()\n",
        "useful_waps = wap_std[wap_std > 0.01].index.tolist()\n",
        "df = df[useful_waps + location_columns + building_floor_columns]\n",
        "\n",
        "# One-hot encode BUILDINGID and FLOOR\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "building_floor_onehot = encoder.fit_transform(df[building_floor_columns])\n",
        "onehot_columns = encoder.get_feature_names_out(building_floor_columns)\n",
        "df[onehot_columns] = building_floor_onehot\n",
        "\n",
        "# Combine RSSI + one-hot features\n",
        "feature_columns = useful_waps + list(onehot_columns)\n",
        "X_all = df[feature_columns].values\n",
        "Y_all = df[location_columns].values\n",
        "\n",
        "# === Generate Enhanced Trajectories ===\n",
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "T = 20  # best sequence length\n",
        "v_max = 1.5  # m/s\n",
        "delta_t = 1.0\n",
        "max_dist = v_max * delta_t\n",
        "\n",
        "dist_matrix = euclidean_distances(Y_all, Y_all)\n",
        "X_sequences = []\n",
        "Y_sequences = []\n",
        "\n",
        "for _ in range(5000):\n",
        "    start_idx = np.random.randint(0, len(df))\n",
        "    traj_x = [X_all[start_idx]]\n",
        "    traj_y = [Y_all[start_idx]]\n",
        "    current_idx = start_idx\n",
        "\n",
        "    for _ in range(T - 1):\n",
        "        neighbors = np.where(dist_matrix[current_idx] <= max_dist)[0]\n",
        "        if len(neighbors) == 0:\n",
        "            break\n",
        "        next_idx = np.random.choice(neighbors)\n",
        "        traj_x.append(X_all[next_idx])\n",
        "        traj_y.append(Y_all[next_idx])\n",
        "        current_idx = next_idx\n",
        "\n",
        "    if len(traj_x) == T:\n",
        "        X_sequences.append(traj_x)\n",
        "        Y_sequences.append(traj_y)\n",
        "\n",
        "X_sequences = np.array(X_sequences)\n",
        "Y_sequences = np.array(Y_sequences)\n",
        "\n",
        "# === Normalize Inputs ===\n",
        "X = X_sequences\n",
        "Y = Y_sequences\n",
        "\n",
        "num_rssi = len(useful_waps)\n",
        "X[:, :, :num_rssi] = (X[:, :, :num_rssi] + 105) / 105  # Normalize RSSI\n",
        "\n",
        "Y_min = Y.min(axis=(0, 1))\n",
        "Y_max = Y.max(axis=(0, 1))\n",
        "Y_scaled = (Y - Y_min) / (Y_max - Y_min)\n",
        "\n",
        "# === Train/Test Split ===\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X, Y_scaled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gXb6qSEfdBLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "idx = random.randint(0, X_val.shape[0] - 1)\n",
        "sample_input = X_val[idx:idx+1]  # shape: (1, T, features)\n",
        "\n",
        "# Predict scaled coordinates\n",
        "pred_scaled = model.predict(sample_input)\n",
        "\n",
        "# Rescale to real-world coordinates\n",
        "pred_coords = pred_scaled * (Y_max - Y_min) + Y_min\n",
        "true_coords = Y_val[idx:idx+1] * (Y_max - Y_min) + Y_min\n",
        "\n",
        "# Print results\n",
        "for t in range(sample_input.shape[1]):\n",
        "    print(f\"Step {t+1}: True = {true_coords[0][t]}, Predicted = {pred_coords[0][t]}\")\n"
      ],
      "metadata": {
        "id": "LsroW7FQcEra",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j_E2178jccFr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}